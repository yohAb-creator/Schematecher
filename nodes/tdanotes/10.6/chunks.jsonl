{"chunk_id": "10.6-0", "text": "Chapter 10. Applications Introduction to TDA Figure 10.18: Persistence diagram for a point on the palm. Figure 10.19: Persistence diagram for a point on a finger. 10.6 TDA in ML Many machine learning pipelines require input points to be in Euclidean space (and not just any metric space, which the persistence diagrams would already fulfill), or in the case of kernel methods, at least in some space that has an inner product, also known as a Hilbert space. Therearemanywaystoturnpersistencediagramsintoelementsofsuchmetricspaces. These methods are also called vectorizations. In this section we introduce three such methods. On https://persistent-homology.streamlit.app you can see more examples and play around with various vectorization methods. Persistence Statistics For the persistence statistics, as the name suggests, we just sum- marize some statistical values of the persistence diagrams. In particular, for the birth times b, the death times d, the interval midpoints b+d and the interval lengths d \u2212 b 2 we record \u2022 the mean, \u2022 the standard deviation, \u2022 the median, \u2022 the interquartile range, 116 Introduction to TDA 10.6. TDA in ML Figure 10.20: The persistence landscape of a persistence diagram. \u2022 the full range and \u2022 four percentiles (10%, 25%, 75%, 90%). Finally we also record the number of bars and the entropy. All in all, we record 38 numerical values, and we thus represent a persistence diagram as a point in R38. PersistenceLandscapes Themainintuitionbehindpersistencelandscapesisthefollowing: for each point in the persistence diagram draw a horizontal and a vertical line segment to the diagonal. Flipping this arrangement of line segments such that the diagonal is horizontal, wegetsomethingthatlookslikeamountainrange, hencethenamelandscape. We can also interpret this as a set of piece-wise linear functions by looking at envelopes: a point on one of the segments is on the k\u2019th envelope of the arrangement if there are k\u22121 segments strictly above it. Each such envelope is now a piece-wise linear function. See Figure 10.20 for an illustration. More formally, let D = {(b ,d ),...,(b ,d )} be a persistence diagram with finitely 1 1 n n many off-diagonal points. Each off-diagonal point (b ,d ) gives rise to a triangle whose i i boundary is defined by the points {(t,min[t\u2212b ,d \u2212t] ) | t \u2208 R }, i i + where min[a,b] := max(0,min(a,b)). + The persistence landscape of D is now a function \u03bb : N\u00d7R \u2192 R defined by D \u03bb (k,t) := k\u2019th largest value of min[t\u2212b ,d \u2212t] for i \u2208 {1,...,n}. D i i + For each k, we have that \u03bb (k,\u00b7) is a piece-wise linear function. Recall that on D functions, we have the following p-norms: (cid:90) (cid:18) (cid:19)1/p \u2225f\u2225 := |f(x)|pdx . p R 117 Chapter 10. Applications Introduction to TDA Figure 10.21: The Betti curve of a persistence diagram. We can extend this to a norm on our set of piece-wise linear functions: \uf8eb \uf8f61/p (cid:88)\u221e \u2225\u03bb D \u2225 p := \uf8ed \u2225\u03bb D (k,\u00b7)\u2225 p\uf8f8 . k=1 In particular, for p = 2 this is a Hilbert space and thus usable for many machine learning pipelines. We can further define the following distance measure on persistence diagrams, called the p-landscape distance \u039b : p \u039b (D ,D ) := \u2225\u03bb \u2212\u03bb \u2225 . p 1 2 D D p 1 2 Taking p = \u221e we get the following Theorem 10.1. Let D and D be persistence diagrams with finitely many off-diagonal 1 2 points. Then \u039b (D ,D ) \u2a7d d (D ,D ). \u221e 1 2 b 1 2 Betti Curves An easier way to get a piece-wise linear (even piece-wise constant) function from a persistence diagram is", "rank": 0, "metadata": {"section": "10.6"}}
{"chunk_id": "10.6-1", "text": "2 Taking p = \u221e we get the following Theorem 10.1. Let D and D be persistence diagrams with finitely many off-diagonal 1 2 points. Then \u039b (D ,D ) \u2a7d d (D ,D ). \u221e 1 2 b 1 2 Betti Curves An easier way to get a piece-wise linear (even piece-wise constant) function from a persistence diagram is through Betti curves: the Betti curve of a persistence diagram is the function \u03b2 : R \u2192 N\u2a7e0 which assigns each time t the current Betti number of the filtration. This is a piece-wise constant function and thus the space of all Betti curves is a Hilbert space with the standard 2-norm. A Betti curve still captures all the births and deaths, but throws away the pairing between them, see Figure 10.21 for an illustration. Questions 39. How can we use TDA to analyze the space of image patches? Discuss the structure of the relevant data set and how persistent homology can be applied. 118 Introduction to TDA 10.6. TDA in ML 40. Why does persistent homology sometimes fail to capture voids in the data? Illustrate the problem with the voting data from the US house of representatives and explain how it can be solved in practice. 41. How can persistence diagrams be used in machine learning pipelines? Explain the following vectorizations: persistence statistics, persistence landscapes and Betti curves. References [1] GunnarCarlsson,TigranIshkhanov,VinDeSilva,andAfraZomorodian,Onthelocal behavior of spaces of natural images. International journal of computer vision, 76, (2008), 1\u201312. [2] Gunnar Carlsson and Mikael Vejdemo-Johansson, Topological Data Analysis with Applications, Cambridge University Press, 2021. [3] Mathieu Carri\u00e8re, Steve Y. Oudot, and Maks Ovsjanikov, Stable Topological Signa- tures for Points on 3D Shapes. Computer Graphics Forum, doi:10.1111/cgf.12692. [4] Rupert G Miller, Discussion: Projection Pursuit. The Annals of Statistics, 13/2, (1985), 510\u2013513. [5] MonicaNicolau,ArnoldJLevine,andGunnarCarlsson,Topologybaseddataanalysis identifies a subgroup of breast cancers with a unique mutational profile and excellent survival. Proceedings of the National Academy of Sciences, 108/17, (2011), 7265\u2013 7270. [6] Gurjeet Singh, Facundo M\u00e9moli, and Gunnar Carlsson, Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. [7] Marc J Van De Vijver, Yudong D He, Laura J Van\u2019t Veer, Hongyue Dai, Augusti- nus AM Hart, Dorien W Voskuil, George J Schreiber, Johannes L Peterse, Chris Roberts, Matthew J Marton et al., A gene-expression signature as a predictor of survival in breast cancer. New England Journal of Medicine, 347/25, (2002), 1999\u2013 2009. [8] J Hans Van Hateren and Arjen van der Schaaf, Independent component filters of natural images compared with simple cells in primary visual cortex. Proceedings of the Royal Society of London. Series B: Biological Sciences, 265/1394, (1998), 359\u2013366. [9] Mikael Vejdemo-Johansson, Florian T Pokorny, Primoz Skraba, and Danica Kragic, Cohomologicallearningofperiodicmotion.Applicable algebra in engineering, com- munication and computing, 26/1, (2015), 5\u201326. 119", "rank": 1, "metadata": {"section": "10.6"}}
